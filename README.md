<div align="center">
<!--   <a href="https://lookerstudio.google.com/reporting/da5da0af-4be0-4f7d-a84b-f7c2892df612"> -->
    <img src="AzurePipeline.gif" alt="Banner" width="720">
  </a>

  <div id="user-content-toc">
    <ul>
      <summary><h1 style="display: inline-block;"> Analyzing Sales of AdventureWorks </h1></summary>
    </ul>
  </div>
  
  <p>On-prem DB to Azure Cloud Pipeline with Data Factory, Lake Storage, Spark, Databricks, Synapse, PowerBI</p>
</div>
<br>

## üìù Table of Contents
1. [Project Overview](#introduction)
2. [Key Insights](#key-insights)
3. [Project Architecture](#project-architecture)  
  3.1. [Data Ingestion](#data-ingestion)  
  3.2. [Data Transformation](#data-transformation)  
  3.3. [Data Loading](#data-loading)  
  3.4. [Data Reporting](#data-reporting)
4. [Credits](#credits)
5. [Contact](#contact)

<a name="introduction"></a>
## üî¨ Project Overview 

This an end-to-end data engineering project on the Azure cloud. Where I did data ingestion from a on-premise SQL Server to Azure Data Lake using Data Factory to transformation using Databricks and Spark, loading to Synapse, and reporting using PowerBI. Also, I used Azure Active Directory (AAD) and Azure Key Vault for the data monitoring and governance purpose. 

### üíæ Dataset

**AdventureWorks** is a database provided by Microsoft for free on online platforms. It is a product sample database originally published by Microsoft to demonstrate the supposed design of a SQL server database using SQL server 2008. Here are some key points to know about AdventureWorks:

- AdventureWorks database supports a manufacturing MNC named Adventure Works Cycles.
- It is a sample Online Transaction Processing (or OLTP) database, which is a type of data processing where multiple transactions occur concurrently. These are shipped by Microsoft with all of their SQL server products.

> For this project I used the **Lightweight (LT) data**: a lightweight and pared down version of the OLTP sample. [Download here](https://github.com/Microsoft/sql-server-samples/releases/download/adventureworks/AdventureWorksLT2022.bak)

### üéØ Project Goals

- Establish a connection between on-premise Postgre server and Azure cloud.
- Ingest tables into the Azure Data Lake.
- Apply data cleaning and transformation using Azure Databricks.
- Utilize Azure Synapse Analytics for loading clean data.
- Create interactive data visualizations and reports with Microsoft Power BI.
- Implement Azure Active Directory (AAD) and Azure Key Vault for monitoring and governance.

<a name="key-insights"></a>
## üïµÔ∏è Key Insights

- üí∏ **Total Revenue by Product Category**
  - *Touring Bikes* is the top 1 category generating revenue with 32% followed by *Road Bikes* with 26% and *Mountain Bikes* with 24%.
 
- üåç **Sales by Country**
  - **N¬∞1:** The United Kingdom (UK) have the most total sales with 278 and $572,000 of total revenue.
  - **N¬∞2:** The United States of America (USA) is second with total sales with 264 and $383,810 of total revenue.

- üöª **Revenue by Gender**
  - 81% of the revenue is generated by Male customers
  - 19% of the revenue is generated by Female customers  

> This can be explained by males have more interest in doing outdoor activites with the different categories of Bikes than females.

<a name="project-architecture"></a>
## üìù Project Architecture

You can find the detailed information on the diagram below:

! AzurePipeline.gif

<a name="data-ingestion"></a>
### üì§ Data Ingestion
- Connected the on-premise SQL Server with Azure using Microsoft Integration Runtime.

! https://github.com/Glavian3/Azure-Data-Engineering-End-to-End/blob/6dfff9c326c51865077597fab75af3a2057c0526/Azure%20ingestion%20runtime.png


- Migrated the tables from on-premise Postgre Server to Azure Data Lake Storage Gen2.

!https://github.com/Glavian3/Azure-Data-Engineering-End-to-End/blob/38600ac695816288452657c6d850d3b9eae65ae9/conatiner.png
! https://github.com/Glavian3/Azure-Data-Engineering-End-to-End/blob/38600ac695816288452657c6d850d3b9eae65ae9/df-pipeline.PNG

<a name="data-transformation"></a>
### ‚öôÔ∏è Data Transformation
- Mounted Azure Blob Storage to Databricks to retrieve raw data from the Data Lake.
- Used Spark Cluster in Azure Databricks to clean and refine the raw data.
- Saved the cleaned data in a Delta format; optimized for further analysis.

!https://github.com/Glavian3/Azure-Data-Engineering-End-to-End/blob/95178f797f66569d34b4de225b77176b3451f82a/Datbricks-Transformation.png

<a name="data-loading"></a>
### üì• Data Loading
- Used Azure Synapse Analytics to load the refined data efficiently.
- Created SQL database and connected it to the data lake.

!https://github.com/Glavian3/Azure-Data-Engineering-End-to-End/blob/95178f797f66569d34b4de225b77176b3451f82a/synapse-pipeline.PNG
!https://github.com/Glavian3/Azure-Data-Engineering-End-to-End/blob/95178f797f66569d34b4de225b77176b3451f82a/db-synapse.PNG
<a name="data-reporting"></a>
### üìä Data Reporting
- Connected Microsoft Power BI to Azure Synapse, and used the Views of the DB to create interactive and insightful data visualizations.

!https://github.com/Glavian3/Azure-Data-Engineering-End-to-End/blob/95178f797f66569d34b4de225b77176b3451f82a/PowerBI-dashboard.PNG

### üõ†Ô∏è Technologies Used

- **Data Source**: PostgreSQL
- **Orchestration**: Azure Data Factory
- **Ingestion**: Azure Data Lake Gen2
- **Storage**: Azure Synapse Analytics
- **Authentication and Secrets Management**: Azure Active Directory and Azure Key Vault
- **Data Visualization**: PowerBI
